{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/emmanuele/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /Users/emmanuele/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_phrase(text):\n",
    "    \"\"\"\n",
    "    Analyze a phrase for company name extraction\n",
    "    Returns (phrase, number_of_proper_nouns, word count)\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    proper_nouns = [word for word, tag in tagged if tag == 'NNP']\n",
    "    is_single_word = len(tagged) == 1 and tagged[0][1].startswith('NN')\n",
    "    \n",
    "    if proper_nouns:\n",
    "        return ' '.join(proper_nouns), len(proper_nouns), len(tokens)\n",
    "    elif is_single_word:\n",
    "        return tagged[0][0], 0, len(tokens)\n",
    "    \n",
    "    return None, 0, len(tokens)\n",
    "    \n",
    "\n",
    "def extract_company_name(text):\n",
    "    \"\"\"\n",
    "    Extract company name from text using NLTK POS tagging\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text containing company name\n",
    "    Returns:\n",
    "        str: Most likely company name\n",
    "    \"\"\"\n",
    "    parts = [part.strip() for part in text.split('|')]\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    \n",
    "    left_phrase, left_proper_count, left_word_count = analyze_phrase(parts[0])\n",
    "    right_phrase, right_proper_count, right_word_count = analyze_phrase(parts[1])\n",
    "        \n",
    "    #  If one side has all NNPs return it and if both have return the shorter one.\n",
    "    if left_proper_count == left_word_count and right_proper_count == right_word_count:\n",
    "        return left_phrase if left_word_count < right_word_count else right_phrase\n",
    "\n",
    "    if left_proper_count == left_word_count:\n",
    "        return left_phrase\n",
    "    if right_proper_count == right_word_count:\n",
    "        return right_phrase\n",
    "\n",
    "    # If not all NNPs, return the shorter one\n",
    "    return left_phrase if left_word_count < right_word_count else right_phrase\n",
    "\n",
    "\n",
    "def text_cleaner(text):\n",
    "    \"\"\"Cleans and normalizes text by removing special characters and formatting.\n",
    "    \n",
    "    Processes text through the following steps:\n",
    "    1. Splits text into sentences\n",
    "    2. Removes all non-alphanumeric characters\n",
    "    3. Strips whitespace\n",
    "    4. Joins sentences with commas\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text string to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text with sentences joined by commas\n",
    "        \n",
    "    Example:\n",
    "        >>> text_cleaner(\"Hello, world! This is a test.\")\n",
    "        \"Hello world,This is a test\"\n",
    "        \n",
    "    Note:\n",
    "        - Preserves only letters, numbers, and spaces\n",
    "        - Removes punctuation, special characters, and extra whitespace\n",
    "        - Maintains sentence boundaries using commas\n",
    "    \"\"\"\n",
    "    # Tokenize text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Replace non-alphanumeric characters with spaces in each sentence\n",
    "    processed_sentences = [re.sub(r'[^a-zA-Z0-9 ]+', ' ', sentence).strip() \n",
    "                         for sentence in sentences]\n",
    "    \n",
    "    # Join sentences with commas\n",
    "    return ','.join(processed_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_cleaner(text):\n",
    "#     \"\"\"Cleans and normalizes text by removing special characters and formatting.\n",
    "    \n",
    "#     Processes text through the following steps:\n",
    "#     1. Splits text into sentences\n",
    "#     2. Removes all non-alphanumeric characters\n",
    "#     3. Strips whitespace\n",
    "#     4. Joins sentences with commas\n",
    "    \n",
    "#     Args:\n",
    "#         text (str): Raw text string to be cleaned\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Cleaned text with sentences joined by commas\n",
    "        \n",
    "#     Example:\n",
    "#         >>> text_cleaner(\"Hello, world! This is a test.\")\n",
    "#         \"Hello world,This is a test\"\n",
    "        \n",
    "#     Note:\n",
    "#         - Preserves only letters, numbers, and spaces\n",
    "#         - Removes punctuation, special characters, and extra whitespace\n",
    "#         - Maintains sentence boundaries using commas\n",
    "#     \"\"\"\n",
    "#     # Tokenize text into sentences\n",
    "#     sentences = sent_tokenize(text)\n",
    "    \n",
    "#     # Replace non-alphanumeric characters with spaces in each sentence\n",
    "#     processed_sentences = [re.sub(r'[^a-zA-Z0-9 ]+', ' ', sentence).strip() \n",
    "#                          for sentence in sentences]\n",
    "    \n",
    "#     # Join sentences with commas\n",
    "#     return ','.join(processed_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SequelSift:\n",
    "#     \"\"\"A class for analyzing company websites to extract key business information.\n",
    "    \n",
    "#     This class provides methods to scrape and analyze company websites, extracting\n",
    "#     information such as company names, descriptions, founder details, and product\n",
    "#     information. It handles URL normalization, page fetching, and HTML parsing.\n",
    "    \n",
    "#     Attributes:\n",
    "#         headers (dict): HTTP headers used for web requests, including user agent\n",
    "        \n",
    "#     Methods:\n",
    "#         analyze_website: Main method to analyze a company's website\n",
    "#         _extract_company_name: Extracts company name from HTML\n",
    "#         _extract_description: Extracts company/product description\n",
    "#         _find_founders: Extracts founder information\n",
    "#         _extract_product_info: Extracts product-related information\n",
    "#         _find_about_page: Locates company's about/team page\n",
    "        \n",
    "#     Example Usage:\n",
    "#         analyzer = SequelSift()\n",
    "#         result = analyzer.analyze_website('example.com')\n",
    "#         print(result['company_name'])\n",
    "#         print(result['description'])\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         \"\"\"_summary_\n",
    "#         \"\"\"\n",
    "#         self.headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "#         }\n",
    "    \n",
    "#     def analyze_website(self, domain):\n",
    "#         \"\"\"Analyzes a website to extract company information.\n",
    "        \n",
    "#         Fetches and parses the main page and about page (if found) to extract\n",
    "#         company details including name, description, founders, and product information.\n",
    "        \n",
    "#         Args:\n",
    "#             domain (str): The website domain to analyze (e.g., 'example.com')\n",
    "            \n",
    "#         Returns:\n",
    "#             dict: Dictionary containing extracted information with keys:\n",
    "#                 - domain: Normalized website URL\n",
    "#                 - company_name: Extracted company name\n",
    "#                 - description: Company/product description\n",
    "#                 - founders: Set of founder names\n",
    "#                 - product_info: Dictionary of product features and details\n",
    "#         \"\"\"\n",
    "#         result = {\n",
    "#             'domain': None,\n",
    "#             'company_name': None,\n",
    "#             'description': None,\n",
    "#             'founders': None,\n",
    "#             'product_info': None\n",
    "#         }\n",
    "#         try:\n",
    "#             # Ensure domain has proper format\n",
    "#             if not domain.startswith(('http://', 'https://')):\n",
    "#                 if not domain.startswith('www.'):\n",
    "#                     domain = 'www.' + domain\n",
    "#                 domain = 'https://' + domain\n",
    "            \n",
    "#             # Fetch main page\n",
    "#             response = requests.get(domain, headers=self.headers, timeout=10)\n",
    "#             soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "#             result['domain'] = domain\n",
    "#             result['company_name'] = self._extract_company_name(soup)\n",
    "#             result['description'] = self._extract_description(soup)\n",
    "#             result['founders'] = self._find_founders(soup)\n",
    "#             result['product_info'] = self._extract_product_info(soup)\n",
    "            \n",
    "#             # Try to find additional pages\n",
    "#             about_page = self._find_about_page(domain, soup)\n",
    "#             if about_page:\n",
    "#                 about_soup = BeautifulSoup(requests.get(about_page, headers=self.headers).text, 'html.parser')\n",
    "#                 result['founders'].update(self._find_founders(about_soup))\n",
    "                \n",
    "#             return result\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f'error, {e}')\n",
    "#             return result\n",
    "    \n",
    "#     def _extract_company_name(self, soup):\n",
    "#         \"\"\"Extracts company name from webpage HTML content.\n",
    "        \n",
    "#         Attempts to find company name from multiple sources in HTML:\n",
    "#         1. Meta tags (og:site_name)\n",
    "#         2. Page title tag\n",
    "        \n",
    "#         Args:\n",
    "#             soup (BeautifulSoup): Parsed HTML content in BeautifulSoup format\n",
    "            \n",
    "#         Returns:\n",
    "#             str | None: First found company name from potential sources,\n",
    "#                     or None if no company name could be extracted\n",
    "#         \"\"\"\n",
    "#         potential_names = []\n",
    "        \n",
    "#         # Check meta tags\n",
    "#         meta_title = soup.find('meta', property='og:site_name')\n",
    "#         if meta_title:\n",
    "#             company_name = meta_title['content']\n",
    "#             potential_names.append(company_name)\n",
    "            \n",
    "#         # Check main title\n",
    "#         title = soup.find('title')\n",
    "#         if title:\n",
    "#             company_name = extract_company_name(title.text)\n",
    "#             potential_names.append(company_name)\n",
    "            \n",
    "#         return potential_names[0] if potential_names else None\n",
    "    \n",
    "#     def _extract_description(self, soup):\n",
    "#         \"\"\"Extracts website description from webpage HTML content.\n",
    "        \n",
    "#         Searches for description in following priority order:\n",
    "#         1. Meta description tag\n",
    "#         2. First paragraph text\n",
    "        \n",
    "#         The extracted text is cleaned before being returned.\n",
    "        \n",
    "#         Args:\n",
    "#             soup (BeautifulSoup): Parsed HTML content in BeautifulSoup format\n",
    "            \n",
    "#         Returns:\n",
    "#             str | None: Cleaned description text if found,\n",
    "#                     None if no description could be extracted\n",
    "#         \"\"\"\n",
    "#         # Try to find meta description\n",
    "#         meta_desc = soup.find('meta', {'name': 'description'})\n",
    "#         if meta_desc:\n",
    "#             return text_cleaner(meta_desc.get('content'))\n",
    "            \n",
    "#         # Try to find first meaningful paragraph\n",
    "#         first_p = soup.find('p')\n",
    "#         if first_p:\n",
    "#             return text_cleaner(first_p.text.strip())\n",
    "            \n",
    "#         return None\n",
    "    \n",
    "#     def _find_founders(self, soup):\n",
    "#         \"\"\"Extracts founder names from webpage HTML content.\n",
    "        \n",
    "#         Searches through various HTML elements (p, div, headers) for founder-related \n",
    "#         keywords and attempts to extract associated names. Looks for text patterns \n",
    "#         where names typically appear before founder-related titles.\n",
    "        \n",
    "#         Args:\n",
    "#             soup (BeautifulSoup): Parsed HTML content in BeautifulSoup format\n",
    "            \n",
    "#         Returns:\n",
    "#             set[str] | None: Set of cleaned founder names if found,\n",
    "#                             None if no founders could be identified or on error\n",
    "                            \n",
    "#         Example extracted patterns:\n",
    "#             \"John Smith, Founder\"\n",
    "#             \"Jane Doe, CEO\"\n",
    "#             \"Bob Wilson, Co-Founder & CTO\"\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             founders = set()\n",
    "#             founder_keywords = ['founder', 'co-founder', 'ceo', 'chief executive']\n",
    "            \n",
    "#             # Look for team sections or about sections\n",
    "#             for elem in soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "#                 text = elem.text.lower()\n",
    "#                 if any(keyword in text for keyword in founder_keywords):\n",
    "#                     # Simple approach to extract names\n",
    "#                     words = text.split()\n",
    "#                     for i in range(len(words)-1):\n",
    "#                         if any(keyword in words[i] for keyword in founder_keywords):\n",
    "#                             # Look for name before the founder keyword\n",
    "#                             potential_name = ' '.join(words[max(0, i-2):i]).strip()\n",
    "#                             if potential_name and len(potential_name.split()) >= 2:\n",
    "#                                 founders.add(text_cleaner(potential_name))\n",
    "                                \n",
    "#             if founders == {}:\n",
    "#                 return None\n",
    "                \n",
    "#             return founders\n",
    "            \n",
    "#         except Exception:\n",
    "#             return None\n",
    "    \n",
    "#     def _extract_product_info(self, soup):\n",
    "#         \"\"\"Extracts product-related information from webpage HTML content.\n",
    "        \n",
    "#         Searches for product information in three main areas:\n",
    "#         1. Feature headers (class='feature-header')\n",
    "#         2. Product block details (class='product-block-details')\n",
    "#         3. Product list titles (class='product-list-title')\n",
    "        \n",
    "#         Args:\n",
    "#             soup (BeautifulSoup): Parsed HTML content in BeautifulSoup format\n",
    "            \n",
    "#         Returns:\n",
    "#             dict[str, list[str]]: Dictionary containing product information with keys:\n",
    "#                 - products: List of product names/titles\n",
    "#                 - features: List of product features/highlights\n",
    "#                 - descriptions: List of product descriptions\n",
    "                \n",
    "#         Note:\n",
    "#             Duplicates are removed while preserving the order of discovery.\n",
    "#             All text values are stripped of leading/trailing whitespace.\n",
    "#         \"\"\"\n",
    "#         product_info = {\n",
    "#             'products': [],\n",
    "#             'features': [],\n",
    "#             'descriptions': []\n",
    "#         }\n",
    "        \n",
    "#         # Extract from feature headers\n",
    "#         feature_headers = soup.find_all('div', class_='feature-header')\n",
    "#         for header in feature_headers:\n",
    "#             h3 = header.find('h3')\n",
    "#             if h3:\n",
    "#                 product_info['products'].append(h3.text.strip())\n",
    "                \n",
    "#         # Extract from product block details\n",
    "#         product_blocks = soup.find_all('div', class_='product-block-details')\n",
    "#         for block in product_blocks:\n",
    "#             title = block.find('h3', class_='product-block-title')\n",
    "#             if title:\n",
    "#                 product_info['products'].append(title.text.strip())\n",
    "                \n",
    "#         # Extract from product list titles\n",
    "#         list_titles = soup.find_all('div', class_='product-list-title')\n",
    "#         for title_block in list_titles:\n",
    "#             h2 = title_block.find('h2')\n",
    "#             p = title_block.find('p')\n",
    "#             if h2:\n",
    "#                 product_info['features'].append(h2.text.strip())\n",
    "#             if p:\n",
    "#                 product_info['descriptions'].append(p.text.strip())\n",
    "                \n",
    "#         # Remove duplicates while preserving order\n",
    "#         for key in product_info:\n",
    "#             product_info[key] = list(dict.fromkeys(product_info[key]))\n",
    "            \n",
    "#         return product_info\n",
    "        \n",
    "        \n",
    "#     def _find_about_page(self, base_url, soup):\n",
    "#         \"\"\"Finds the URL of the company's about or team page.\n",
    "        \n",
    "#         Searches for links containing 'about' or 'team' in their href attributes\n",
    "#         (case-insensitive) and constructs the full URL using the base URL.\n",
    "        \n",
    "#         Args:\n",
    "#             base_url (str): The website's base URL (e.g., 'https://example.com')\n",
    "#             soup (BeautifulSoup): Parsed HTML content in BeautifulSoup format\n",
    "            \n",
    "#         Returns:\n",
    "#             str | None: Full URL of the about/team page if found,\n",
    "#                     None if no relevant page could be found\n",
    "                    \n",
    "#         Example:\n",
    "#             base_url: 'https://example.com'\n",
    "#             found href: '/about-us'\n",
    "#             returns: 'https://example.com/about-us'\n",
    "#         \"\"\"\n",
    "#         about_links = soup.find_all('a', href=re.compile(r'about|team', re.I))\n",
    "#         if about_links:\n",
    "#             return urljoin(base_url, about_links[0]['href'])\n",
    "#         return None\n",
    "\n",
    "\n",
    "# company_domains = [\n",
    "#         'tonestro.com',\n",
    "#         'sendtrumpet.com',\n",
    "#         'prewave.com',\n",
    "#         'twinn.health',\n",
    "#         'kokoon.io'\n",
    "#     ]\n",
    "\n",
    "\n",
    "# def main(domains):\n",
    "#     \"\"\" Run scraping on a list of websites.\n",
    "#     Args:\n",
    "#         domains (list): list of strings of company domains.\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(\"Sequel Sift -- Extracting Startup data...\")\n",
    "#     analyzer = SequelSift()\n",
    "    \n",
    "#     results = [analyzer.analyze_website(domain) for domain in domains]\n",
    "        \n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     company_infos = main(company_domains)\n",
    "#     display(company_infos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequel Sift -- Extracting Startup data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>company_name</th>\n",
       "      <th>description</th>\n",
       "      <th>founders</th>\n",
       "      <th>product_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.tonestro.com</td>\n",
       "      <td>tonestro</td>\n",
       "      <td>Learn to play wind instruments with tonestro  ...</td>\n",
       "      <td>{christoph huber, christian kapplm ller, alexa...</td>\n",
       "      <td>{'products': ['Brass &amp; Woodwind Lessons by ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.sendtrumpet.com</td>\n",
       "      <td>trumpet</td>\n",
       "      <td>Auto personalised   interactive digital sales ...</td>\n",
       "      <td>{andrew  exited}</td>\n",
       "      <td>{'products': [], 'features': [], 'descriptions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.prewave.com</td>\n",
       "      <td>Prewave</td>\n",
       "      <td>Supplier monitoring for purchasing  supply cha...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'products': [], 'features': [], 'descriptions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.twinn.health</td>\n",
       "      <td>Twinn Health</td>\n",
       "      <td>Twinn  AI powered insights for your health s f...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'products': [], 'features': [], 'descriptions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.kokoon.io</td>\n",
       "      <td>Kokoon</td>\n",
       "      <td>Philips Sleep Headphones with Kokoon   Wear fo...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'products': ['Philips Sleep Headphones with K...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        domain  company_name  \\\n",
       "0     https://www.tonestro.com      tonestro   \n",
       "1  https://www.sendtrumpet.com       trumpet   \n",
       "2      https://www.prewave.com       Prewave   \n",
       "3     https://www.twinn.health  Twinn Health   \n",
       "4        https://www.kokoon.io        Kokoon   \n",
       "\n",
       "                                         description  \\\n",
       "0  Learn to play wind instruments with tonestro  ...   \n",
       "1  Auto personalised   interactive digital sales ...   \n",
       "2  Supplier monitoring for purchasing  supply cha...   \n",
       "3  Twinn  AI powered insights for your health s f...   \n",
       "4  Philips Sleep Headphones with Kokoon   Wear fo...   \n",
       "\n",
       "                                            founders  \\\n",
       "0  {christoph huber, christian kapplm ller, alexa...   \n",
       "1                                   {andrew  exited}   \n",
       "2                                                 {}   \n",
       "3                                                 {}   \n",
       "4                                                 {}   \n",
       "\n",
       "                                        product_info  \n",
       "0  {'products': ['Brass & Woodwind Lessons by ton...  \n",
       "1  {'products': [], 'features': [], 'descriptions...  \n",
       "2  {'products': [], 'features': [], 'descriptions...  \n",
       "3  {'products': [], 'features': [], 'descriptions...  \n",
       "4  {'products': ['Philips Sleep Headphones with K...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class SequelSift:\n",
    "    \"\"\"A class for analyzing company websites to extract key business information.\n",
    "    \n",
    "    This class provides methods to scrape and analyze company websites, extracting\n",
    "    information such as company names, descriptions, founder details, and product\n",
    "    information. It handles URL normalization, page fetching, and HTML parsing.\n",
    "    \n",
    "    Attributes:\n",
    "        headers (dict): HTTP headers used for web requests, including user agent\n",
    "        \n",
    "    Methods:\n",
    "        analyze_website: Main method to analyze a company's website\n",
    "        _extract_company_name: Extracts company name from HTML\n",
    "        _extract_description: Extracts company/product description\n",
    "        _find_founders: Extracts founder information\n",
    "        _extract_product_info: Extracts product-related information\n",
    "        _find_about_page: Locates company's about/team page\n",
    "        \n",
    "    Example Usage:\n",
    "        analyzer = SequelSift()\n",
    "        result = analyzer.analyze_website('example.com')\n",
    "        print(result['company_name'])\n",
    "        print(result['description'])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with headers and retry settings\"\"\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        self.max_retries = 3\n",
    "        self.retry_delay = 2  # seconds\n",
    "        \n",
    "    def _fetch_with_retry(self, url):\n",
    "        \"\"\"Fetches a webpage with retry logic for reliability.\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL to fetch\n",
    "            \n",
    "        Returns:\n",
    "            BeautifulSoup | None: Parsed HTML content or None if all retries fail\n",
    "        \"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, timeout=10)\n",
    "                response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed for {url}: {str(e)}\")\n",
    "                 # Don't sleep on last attempt\n",
    "                if attempt < self.max_retries - 1: \n",
    "                    print(\"re-attempting extraction\")\n",
    "                    time.sleep(self.retry_delay * (attempt + 1))  # Exponential backoff\n",
    "        return None\n",
    "    \n",
    "    def analyze_website(self, domain: str) -> dict:\n",
    "        \"\"\"Analyzes a website with retry logic for reliability.\"\"\"\n",
    "        result = {\n",
    "            'domain': None,\n",
    "            'company_name': None,\n",
    "            'description': None,\n",
    "            'founders': None,\n",
    "            'product_info': None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Ensure domain has proper format\n",
    "            if not domain.startswith(('http://', 'https://')):\n",
    "                if not domain.startswith('www.'):\n",
    "                    domain = 'www.' + domain\n",
    "                domain = 'https://' + domain\n",
    "            \n",
    "            result['domain'] = domain\n",
    "            \n",
    "            # Fetch main page with retry\n",
    "            soup = self._fetch_with_retry(domain)\n",
    "            if soup is None:\n",
    "                print(f\"Failed to fetch {domain} after {self.max_retries} attempts\")\n",
    "                return result\n",
    "                \n",
    "            # Extract information\n",
    "            result['company_name'] = self._extract_company_name(soup)\n",
    "            result['description'] = self._extract_description(soup)\n",
    "            result['founders'] = self._find_founders(soup)\n",
    "            result['product_info'] = self._extract_product_info(soup)\n",
    "            \n",
    "            # Try to find and fetch about page\n",
    "            about_page = self._find_about_page(domain, soup)\n",
    "            if about_page:\n",
    "                about_soup = self._fetch_with_retry(about_page)\n",
    "                if about_soup and result['founders'] is not None:\n",
    "                    result['founders'].update(self._find_founders(about_soup))\n",
    "                    \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error analyzing {domain}: {str(e)}')\n",
    "            return result\n",
    "        \n",
    "        \n",
    "    def _extract_company_name(self, soup):\n",
    "        \"\"\"Extracts company name from webpage HTML content.\n",
    "        \n",
    "        Attempts to find company name from multiple sources in HTML:\n",
    "        1. Meta tags (og:site_name)\n",
    "        2. Page title tag\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML content in BeautifulSoup format\n",
    "            \n",
    "        Returns:\n",
    "            str | None: First found company name from potential sources,\n",
    "                    or None if no company name could be extracted\n",
    "        \"\"\"\n",
    "        potential_names = []\n",
    "        \n",
    "        # Check meta tags\n",
    "        meta_title = soup.find('meta', property='og:site_name')\n",
    "        if meta_title:\n",
    "            company_name = meta_title['content']\n",
    "            potential_names.append(company_name)\n",
    "            \n",
    "        # Check main title\n",
    "        title = soup.find('title')\n",
    "        if title:\n",
    "            company_name = extract_company_name(title.text)\n",
    "            potential_names.append(company_name)\n",
    "            \n",
    "        return potential_names[0] if potential_names else None\n",
    "    \n",
    "    def _extract_description(self, soup):\n",
    "        \"\"\"Extracts website description from webpage HTML content.\n",
    "        \n",
    "        Searches for description in following priority order:\n",
    "        1. Meta description tag\n",
    "        2. First paragraph text\n",
    "        \n",
    "        The extracted text is cleaned before being returned.\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML content in BeautifulSoup format\n",
    "            \n",
    "        Returns:\n",
    "            str | None: Cleaned description text if found,\n",
    "                    None if no description could be extracted\n",
    "        \"\"\"\n",
    "        # Try to find meta description\n",
    "        meta_desc = soup.find('meta', {'name': 'description'})\n",
    "        if meta_desc:\n",
    "            return text_cleaner(meta_desc.get('content'))\n",
    "            \n",
    "        # Try to find first meaningful paragraph\n",
    "        first_p = soup.find('p')\n",
    "        if first_p:\n",
    "            return text_cleaner(first_p.text.strip())\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def _find_founders(self, soup):\n",
    "        \"\"\"Extracts founder names from webpage HTML content.\n",
    "        \n",
    "        Searches through various HTML elements (p, div, headers) for founder-related \n",
    "        keywords and attempts to extract associated names. Looks for text patterns \n",
    "        where names typically appear before founder-related titles.\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML content in BeautifulSoup format\n",
    "            \n",
    "        Returns:\n",
    "            set[str] | None: Set of cleaned founder names if found,\n",
    "                            None if no founders could be identified or on error\n",
    "                            \n",
    "        Example extracted patterns:\n",
    "            \"John Smith, Founder\"\n",
    "            \"Jane Doe, CEO\"\n",
    "            \"Bob Wilson, Co-Founder & CTO\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            founders = set()\n",
    "            founder_keywords = ['founder', 'co-founder', 'ceo', 'chief executive']\n",
    "            \n",
    "            # Look for team sections or about sections\n",
    "            for elem in soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "                text = elem.text.lower()\n",
    "                if any(keyword in text for keyword in founder_keywords):\n",
    "                    # Simple approach to extract names\n",
    "                    words = text.split()\n",
    "                    for i in range(len(words)-1):\n",
    "                        if any(keyword in words[i] for keyword in founder_keywords):\n",
    "                            # Look for name before the founder keyword\n",
    "                            potential_name = ' '.join(words[max(0, i-2):i]).strip()\n",
    "                            if potential_name and len(potential_name.split()) >= 2:\n",
    "                                founders.add(text_cleaner(potential_name))\n",
    "                                \n",
    "            if founders == {}:\n",
    "                return None\n",
    "                \n",
    "            return founders\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def _extract_product_info(self, soup):\n",
    "        \"\"\"Extracts product-related information from webpage HTML content.\n",
    "        \n",
    "        Searches for product information in three main areas:\n",
    "        1. Feature headers (class='feature-header')\n",
    "        2. Product block details (class='product-block-details')\n",
    "        3. Product list titles (class='product-list-title')\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML content in BeautifulSoup format\n",
    "            \n",
    "        Returns:\n",
    "            dict[str, list[str]]: Dictionary containing product information with keys:\n",
    "                - products: List of product names/titles\n",
    "                - features: List of product features/highlights\n",
    "                - descriptions: List of product descriptions\n",
    "                \n",
    "        Note:\n",
    "            Duplicates are removed while preserving the order of discovery.\n",
    "            All text values are stripped of leading/trailing whitespace.\n",
    "        \"\"\"\n",
    "        product_info = {\n",
    "            'products': [],\n",
    "            'features': [],\n",
    "            'descriptions': []\n",
    "        }\n",
    "        \n",
    "        # Extract from feature headers\n",
    "        feature_headers = soup.find_all('div', class_='feature-header')\n",
    "        for header in feature_headers:\n",
    "            h3 = header.find('h3')\n",
    "            if h3:\n",
    "                product_info['products'].append(h3.text.strip())\n",
    "                \n",
    "        # Extract from product block details\n",
    "        product_blocks = soup.find_all('div', class_='product-block-details')\n",
    "        for block in product_blocks:\n",
    "            title = block.find('h3', class_='product-block-title')\n",
    "            if title:\n",
    "                product_info['products'].append(title.text.strip())\n",
    "                \n",
    "        # Extract from product list titles\n",
    "        list_titles = soup.find_all('div', class_='product-list-title')\n",
    "        for title_block in list_titles:\n",
    "            h2 = title_block.find('h2')\n",
    "            p = title_block.find('p')\n",
    "            if h2:\n",
    "                product_info['features'].append(h2.text.strip())\n",
    "            if p:\n",
    "                product_info['descriptions'].append(p.text.strip())\n",
    "                \n",
    "        # Remove duplicates while preserving order\n",
    "        for key in product_info:\n",
    "            product_info[key] = list(dict.fromkeys(product_info[key]))\n",
    "            \n",
    "        return product_info\n",
    "        \n",
    "        \n",
    "    def _find_about_page(self, base_url, soup):\n",
    "        \"\"\"Finds the URL of the company's about or team page.\n",
    "        \n",
    "        Searches for links containing 'about' or 'team' in their href attributes\n",
    "        (case-insensitive) and constructs the full URL using the base URL.\n",
    "        \n",
    "        Args:\n",
    "            base_url (str): The website's base URL (e.g., 'https://example.com')\n",
    "            soup (BeautifulSoup): Parsed HTML content in BeautifulSoup format\n",
    "            \n",
    "        Returns:\n",
    "            str | None: Full URL of the about/team page if found,\n",
    "                    None if no relevant page could be found\n",
    "                    \n",
    "        Example:\n",
    "            base_url: 'https://example.com'\n",
    "            found href: '/about-us'\n",
    "            returns: 'https://example.com/about-us'\n",
    "        \"\"\"\n",
    "        about_links = soup.find_all('a', href=re.compile(r'about|team', re.I))\n",
    "        if about_links:\n",
    "            return urljoin(base_url, about_links[0]['href'])\n",
    "        return None\n",
    "\n",
    "\n",
    "company_domains = [\n",
    "        'tonestro.com',\n",
    "        'sendtrumpet.com',\n",
    "        'prewave.com',\n",
    "        'twinn.health',\n",
    "        'kokoon.io'\n",
    "    ]\n",
    "\n",
    "\n",
    "def main(domains):\n",
    "    \"\"\" Run scraping on a list of websites.\n",
    "    Args:\n",
    "        domains (list): list of strings of company domains.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Sequel Sift -- Extracting Startup data...\")\n",
    "    analyzer = SequelSift()\n",
    "    \n",
    "    results = [analyzer.analyze_website(domain) for domain in domains]\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_infos = main(company_domains)\n",
    "    display(company_infos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
